{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MistralConfig, MistralForCausalLM\n",
    "import tensorrt_llm\n",
    "from tensorrt_llm.models.modeling_utils import PretrainedConfig\n",
    "\n",
    "from tensorrt_llm import Builder\n",
    "import torch\n",
    "\n",
    "import tensorrt as trt\n",
    "import json\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral_config = MistralConfig.from_pretrained(\"model/models--mistralai--Mistral-7B-v0.1/snapshots/26bca36bde8333b5d7f72e9ed20ccda6a618af24/config.json\")\n",
    "# mistral_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_parallel = 1\n",
    "\n",
    "# config = {\n",
    "#     'architecture': \"LlamaForCausalLM\",\n",
    "#     # 'dtype': torch.bfloat16,\n",
    "#     'dtype': mistral_config.torch_dtype,\n",
    "\n",
    "#     'logits_dtype': 'float32',\n",
    "#     'num_heads': mistral_config.num_attention_heads, \n",
    "#     'num_kv_heads': mistral_config.num_key_value_heads, \n",
    "#     'hidden_size': mistral_config.hidden_size, \n",
    "#     'vocab_size': mistral_config.vocab_size,\n",
    "#     'hidden_act': mistral_config.hidden_act, \n",
    "#     'max_position_embeddings': mistral_config.max_position_embeddings,\n",
    "\n",
    "#     'num_hidden_layers': mistral_config.num_hidden_layers,\n",
    "#     'num_attention_heads': mistral_config.num_attention_heads,\n",
    "#     'hidden_size': mistral_config.hidden_size,\n",
    "#     'intermediate_size': mistral_config.intermediate_size,\n",
    "#     'num_key_value_heads': mistral_config.num_key_value_heads,\n",
    "#     'vocab_size': mistral_config.vocab_size,\n",
    "#     'position_embedding_type': 'rope_gpt_neox',\n",
    "#     'max_position_embeddings': mistral_config.max_position_embeddings,\n",
    "#     'hidden_act': mistral_config.hidden_act,\n",
    "#     'rotary_base': getattr(mistral_config, 'rotary_base', 10000.0),\n",
    "#     'rotary_scaling': getattr(mistral_config, 'rotary_scaling',\n",
    "#                                 None),\n",
    "#     'norm_epsilon': mistral_config.rms_norm_eps,\n",
    "#     'mapping': {\n",
    "#         'world_size': tensor_parallel,\n",
    "#         'tp_size': tensor_parallel,\n",
    "#     },\n",
    "#     'use_parallel_embedding': False,\n",
    "#     'embedding_sharding_dim': 0,\n",
    "#     'use_prompt_tuning': False,\n",
    "#     'moe_num_experts': 0,\n",
    "#     'moe_top_k': 0,\n",
    "#     'moe_tp_mode': 1,\n",
    "#     'moe_normalization_mode': 1,\n",
    "#     'use_fused_mlp': False,\n",
    "#     'enable_pos_shift': False,\n",
    "#     'dense_context_fmha': False,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_config = PretrainedConfig.from_dict(deepcopy(config))\n",
    "# tensorrt_llm_mistral = tensorrt_llm.models.LLaMAForCausalLM(deepcopy(pretrained_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorrt as trt\n",
    "\n",
    "# from collections import namedtuple\n",
    "# one_mapping = dict(\n",
    "#     world_size=tensor_parallel, \n",
    "#     tp_size=tensor_parallel,\n",
    "#     is_first_pp_rank=lambda : True,\n",
    "# )\n",
    "# mapping = namedtuple(\"mapping\", one_mapping.keys())(*one_mapping.values())\n",
    "\n",
    "# tensorrt_llm_mistral = tensorrt_llm.models.LLaMAForCausalLM(\n",
    "#     # PretrainedConfig.from_dict(\n",
    "#         **dict(\n",
    "#         # architecture=\"LlamaForCausalLM\",\n",
    "#         num_layers=mistral_config.num_hidden_layers,\n",
    "#         num_heads=mistral_config.num_attention_heads,\n",
    "#         num_kv_heads=mistral_config.num_key_value_heads,\n",
    "#         hidden_size=mistral_config.hidden_size,\n",
    "#         vocab_size=mistral_config.vocab_size,\n",
    "#         hidden_act=mistral_config.hidden_act,\n",
    "#         max_position_embeddings=mistral_config.max_position_embeddings,\n",
    "#         # dtype=mistral_config.torch_dtype,\n",
    "#         dtype=trt.DataType.BF16,\n",
    "#         logits_dtype=\"float32\",\n",
    "#         position_embedding_type=\"rope_gpt_neox\",\n",
    "#         rotary_base=getattr(mistral_config, 'rotary_base', 10000.0),\n",
    "#         rotary_scaling=getattr(mistral_config, 'rotary_scaling', None),\n",
    "#         mapping=mapping,\n",
    "#         # quant_mode: QuantMode = QuantMode(0),\n",
    "#         use_parallel_embedding=False,\n",
    "#         embedding_sharding_dim=0,\n",
    "#         rms_norm_eps=mistral_config.rms_norm_eps,\n",
    "#         use_fused_mlp=False,\n",
    "#         use_prompt_tuning=False,\n",
    "#     )\n",
    "#     # )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "one_mapping = dict(world_size=tensor_parallel, tp_size=tensor_parallel)\n",
    "mapping = namedtuple(\"mapping\", one_mapping.keys())(*one_mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PretrainedConfig.from_json_file(\"model/mistral_7B_v0.1_instruct_int4_trtllm/mistral_tp1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# config_path = \"model/mistral_7B_v0.1_instruct_int4_trtllm/mistral_tp1.json\"\n",
    "# with open(config_path) as f:\n",
    "#     config = json.load(f)\n",
    "\n",
    "# config[\"architecture\"]= \"LlamaForCausalLM\"\n",
    "# config['num_attention_heads'] = mistral_config.num_attention_heads\n",
    "# config['hidden_size'] = mistral_config.hidden_size\n",
    "# config['num_hidden_layers'] = mistral_config.num_hidden_layers\n",
    "# config['hidden_act'] = mistral_config.hidden_act\n",
    "\n",
    "\n",
    "# # config['max_position_embeddings'] = mistral_config.max_position_embeddings\n",
    "# # config['logits_dtype'] = 'float32'\n",
    "# # config['num_kv_heads'] = mistral_config.num_key_value_heads\n",
    "# # # config['vocab_size'] = mistral_config.vocab_size\n",
    "\n",
    "# # print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorrt_llm_mistral = tensorrt_llm.models.LLaMAModel(PretrainedConfig.from_dict(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization  = config.pop(\n",
    "# 'quantization', \n",
    "# {\n",
    "#     'use_smooth_quant': False,\n",
    "#     'per_channel': False,\n",
    "#     'per_token': False,\n",
    "#     'per_group': False,\n",
    "#     'group_size': 128,\n",
    "#     'int8_kv_cache': False,\n",
    "#     'enable_fp8': False,\n",
    "#     'fp8_kv_cache': False,\n",
    "#     'use_weight_only': False,\n",
    "#     'weight_only_precision': 'int8'\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorrt_llm_mistral = tensorrt_llm.models.LLaMAForCausalLM(PretrainedConfig.from_json_file(\"model/mistral_7B_v0.1_instruct_int4_trtllm/mistral_tp1.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorrt_llm_llama = tensorrt_llm.models.LLaMAForCausalLM(\n",
    "#     num_layers=mistral_config.num_hidden_layers,\n",
    "#     num_heads=mistral_config.num_attention_heads,\n",
    "#     num_kv_heads=mistral_config.num_key_value_heads,\n",
    "#     hidden_size=mistral_config.hidden_size,\n",
    "#     vocab_size=mistral_config.vocab_size,\n",
    "#     hidden_act=mistral_config.hidden_act,\n",
    "#     max_position_embeddings=mistral_config.max_position_embeddings,\n",
    "#     # dtype=mistral_config.torch_dtype,\n",
    "#     dtype=trt.DataType.BF16,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_mistral = MistralForCausalLM.from_pretrained(\n",
    "#     \"model/models--mistralai--Mistral-7B-v0.1/snapshots/26bca36bde8333b5d7f72e9ed20ccda6a618af24\",\n",
    "#     # device_map={\n",
    "#     #     \"model\": \"cpu\",\n",
    "#     #     \"lm_head\": \"cpu\"\n",
    "#     # },  # Load to CPU memory\n",
    "#     torch_dtype=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorrt_llm.models.llama.weight import load_from_hf_llama\n",
    "\n",
    "# load_from_hf_llama(\n",
    "#     tensorrt_llm_llama=tensorrt_llm_llama,\n",
    "#     hf_llama=hf_mistral,\n",
    "#     mapping=Mapping(),\n",
    "#     dtype=\"float16\",\n",
    "#     # use_gemm_woq_plugin=True,\n",
    "#     # lora_config=LoraConfig()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# config_path = \"model/mistral_7B_v0.1_instruct_int4_trtllm/mistral_tp1.json\"\n",
    "# with open(config_path) as f:\n",
    "#     config = json.load(f)\n",
    "\n",
    "# # config[\"architecture\"]= \"LlamaForCausalLM\"\n",
    "# # config['num_attention_heads'] = mistral_config.num_attention_heads\n",
    "# # config['hidden_size'] = mistral_config.hidden_size\n",
    "# # config['num_hidden_layers'] = mistral_config.num_hidden_layers\n",
    "# # config['hidden_act'] = mistral_config.hidden_act\n",
    "\n",
    "\n",
    "# # config['max_position_embeddings'] = mistral_config.max_position_embeddings\n",
    "# # config['logits_dtype'] = 'float32'\n",
    "# # config['num_kv_heads'] = mistral_config.num_key_value_heads\n",
    "# # # config['vocab_size'] = mistral_config.vocab_size\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorrt_llm.models.llama.weight import load_from_awq_llama\n",
    "\n",
    "# tensorrt_llm_llama = tensorrt_llm.models.LLaMAForCausalLM(\n",
    "#     num_layers=mistral_config.num_hidden_layers,\n",
    "#     num_heads=mistral_config.num_attention_heads,\n",
    "#     num_kv_heads=mistral_config.num_key_value_heads,\n",
    "#     hidden_size=mistral_config.hidden_size,\n",
    "#     vocab_size=mistral_config.vocab_size,\n",
    "#     hidden_act=mistral_config.hidden_act,\n",
    "#     max_position_embeddings=mistral_config.max_position_embeddings,\n",
    "#     # dtype=mistral_config.torch_dtype,\n",
    "#     dtype=trt.DataType.BF16,\n",
    "# )\n",
    "\n",
    "# weights = load_from_awq_llama(\n",
    "#     tensorrt_llm_llama,\n",
    "#     quant_ckpt_path=\"model/mistral_7B_v0.1_instruct_int4_trtllm/mistral_tp1_rank0.npz\",\n",
    "#     # vocab_size=mistral_config.vocab_size,\n",
    "#     # quantize_lm_head=False,\n",
    "#     quantize_lm_head=False,\n",
    "#     # mapping=Mapping(),\n",
    "#     # dtype=\"float16\",\n",
    "#     # bin_model_dir=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    mistral_config.max_position_embeddings,\n",
    "    mistral_config.hidden_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMa Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MistralConfig, MistralForCausalLM, LlamaConfig, LlamaForCausalLM\n",
    "import tensorrt_llm\n",
    "from tensorrt_llm.models.modeling_utils import PretrainedConfig\n",
    "from tensorrt_llm import Builder\n",
    "from tensorrt_llm.models.llama.weight import load_from_hf_llama\n",
    "import torch\n",
    "\n",
    "import tensorrt as trt\n",
    "import json\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_parallel = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": null,\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 13824,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 40,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"num_key_value_heads\": 40,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llama_path = \"C:/Users/mehdi/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-chat-hf/snapshots/717c4c9e612e583993155c2b586332e301b61f55/\"\n",
    "llama_gonfig_relpath = \"config.json\"\n",
    "\n",
    "llama_config = LlamaConfig.from_pretrained(llama_path)\n",
    "print(llama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LLaMAForCausalLM.__init__() got an unexpected keyword argument 'intermediate_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tensorrt_llm_llama \u001b[38;5;241m=\u001b[39m \u001b[43mtensorrt_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLaMAForCausalLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dtype=llama_config.torch_dtype,\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFLOAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: LLaMAForCausalLM.__init__() got an unexpected keyword argument 'intermediate_size'"
     ]
    }
   ],
   "source": [
    "tensorrt_llm_llama = tensorrt_llm.models.LLaMAForCausalLM(\n",
    "    num_layers=llama_config.num_hidden_layers,\n",
    "    num_heads=llama_config.num_attention_heads,\n",
    "    num_kv_heads=llama_config.num_key_value_heads,\n",
    "    # intermediate_size=llama_config.intermediate_size,\n",
    "    hidden_size=llama_config.hidden_size,\n",
    "    vocab_size=llama_config.vocab_size,\n",
    "    hidden_act=llama_config.hidden_act,\n",
    "    max_position_embeddings=llama_config.max_position_embeddings,\n",
    "    # dtype=llama_config.torch_dtype,\n",
    "    dtype=trt.DataType.FLOAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]C:\\Users\\mehdi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:39<00:00, 13.27s/it]\n",
      "C:\\Users\\mehdi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mehdi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hf_llama = LlamaForCausalLM.from_pretrained(\n",
    "    llama_path,\n",
    "    # device=\"cpu\",\n",
    "    # device_map={\n",
    "    #     \"model\": \"cpu\",\n",
    "    #     \"lm_head\": \"cpu\"\n",
    "    # },  # Load to CPU memory\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The value updated is not the same shape as the original. Updated: (13824, 5120), original: (20480, 5120)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_hf_llama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorrt_llm_llama\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_llama\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dtype=\"float16\",\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# mapping=tensorrt_llm.Mapping(\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     world_size=tensor_parallel,\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     rank=0,\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     tp_size=tensor_parallel,\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ),\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# mapping=Mapping(),\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dtype='float16',\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use_gemm_woq_plugin=False,\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# lora_config=LoraConfig()\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorrt_llm\\models\\llama\\weight.py:422\u001b[0m, in \u001b[0;36mload_from_hf_llama\u001b[1;34m(tensorrt_llm_llama, hf_llama, mapping, dtype, use_gemm_woq_plugin, lora_config)\u001b[0m\n\u001b[0;32m    420\u001b[0m         scales\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m torch_to_numpy(torch_weight_scales)\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 422\u001b[0m         \u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(split_v)\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperts.w2.weight\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k:\n\u001b[0;32m    424\u001b[0m     dst \u001b[38;5;241m=\u001b[39m tensorrt_llm_llama\u001b[38;5;241m.\u001b[39mlayers[idx]\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mexperts_weight_2\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorrt_llm\\parameter.py:84\u001b[0m, in \u001b[0;36mParameter.value\u001b[1;34m(self, v)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;129m@value\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue\u001b[39m(\u001b[38;5;28mself\u001b[39m, v: Union[np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor]):\n\u001b[0;32m     83\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_regularize_value(v)\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m v\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\u001b[38;5;241m.\u001b[39mshape, \\\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe value updated is not the same shape as the original. \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, original: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m v\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[0;32m     88\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter was initialized as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m         )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The value updated is not the same shape as the original. Updated: (13824, 5120), original: (20480, 5120)"
     ]
    }
   ],
   "source": [
    "weights = load_from_hf_llama(\n",
    "    tensorrt_llm_llama,\n",
    "    hf_llama,\n",
    "    # dtype=\"float16\",\n",
    "    # mapping=tensorrt_llm.Mapping(\n",
    "    #     world_size=tensor_parallel,\n",
    "    #     rank=0,\n",
    "    #     tp_size=tensor_parallel,\n",
    "    # ),\n",
    "    # mapping=Mapping(),\n",
    "    # dtype='float16',\n",
    "    # use_gemm_woq_plugin=False,\n",
    "    # lora_config=LoraConfig()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
